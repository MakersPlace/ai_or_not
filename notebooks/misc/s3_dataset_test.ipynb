{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T02:16:35.046705Z",
     "iopub.status.busy": "2023-09-09T02:16:35.045665Z",
     "iopub.status.idle": "2023-09-09T02:16:35.051482Z",
     "shell.execute_reply": "2023-09-09T02:16:35.050504Z",
     "shell.execute_reply.started": "2023-09-09T02:16:35.046570Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install wandb\n",
    "# ! pip install pydot\n",
    "# ! pip install graphviz\n",
    "# ! pip install datasets\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install sagemaker_tensorflow # uses Linux FIFOs so does not work on Mac\n",
    "# ! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T02:16:35.077597Z",
     "iopub.status.busy": "2023-09-09T02:16:35.076805Z",
     "iopub.status.idle": "2023-09-09T02:16:43.964860Z",
     "shell.execute_reply": "2023-09-09T02:16:43.963905Z",
     "shell.execute_reply.started": "2023-09-09T02:16:35.077562Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from datasets import load_from_disk\n",
    "from IPython.display import Image\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_SUFFIX = \"frequency_classifier_multi\"\n",
    "ENTITY = \"makersplace\"\n",
    "PROJECT = f\"ai-or-not-{PROJECT_SUFFIX}\"\n",
    "SEED = 7\n",
    "RUNTIME_DATE_SUFFIX = \"%m%d_%H%M\"\n",
    "\n",
    "# current time\n",
    "JOB_TYPE_SUFFIX = f\"{PROJECT_SUFFIX}_M\"\n",
    "RUN_NAME_SUFFIX = datetime.now().strftime(RUNTIME_DATE_SUFFIX)\n",
    "\n",
    "# Datasets Paths\n",
    "CACHE_DIRECTORY = \"../cache/tf_datasets\"\n",
    "MISC_DIRECTORY = f\"{CACHE_DIRECTORY}/../misc\"\n",
    "training_dataset_path = f\"{CACHE_DIRECTORY}/training_dataset\"\n",
    "testing_dataset_path = f\"{CACHE_DIRECTORY}/testing_datasets\"\n",
    "download_path = f\"{MISC_DIRECTORY}/downloaded_datasets\"\n",
    "dataset_cache_path = f\"{MISC_DIRECTORY}/s3_dataset.cache\"\n",
    "\n",
    "\n",
    "# S3 Paths\n",
    "S3_BUCKET = \"mp-ml-data-dev\"\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1002_1858/\" # 100 Shards\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1003_1912/\" # 1000 Shards\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1003_0955/\" # 100 Shards\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1003_1812/\" # 10 Shards\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1004_1258/\" # 10 *.tfrecord files\n",
    "# PREFIX = f\"finder/ai_or_not/tf_datasets/1004_1338/\" # 10 GZIP *.tfrecord files\n",
    "PREFIX = f\"finder/ai_or_not/tf_datasets/1004_1419/\"  # 20 GZIP *.tfrecord files\n",
    "\n",
    "\n",
    "# Deleted and recreated training and validation dataset folders\n",
    "CLEAN_RUN = True\n",
    "# Dictates processing all data or just a subset\n",
    "FULL_RUN = True\n",
    "COMPRESSION_TYPE = \"GZIP\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# log to stdout\n",
    "log.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    level=log.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB Login\n",
    "os.environ[\"WANDB_API_KEY\"] = \"d13afab09b400fc9d606e612d806a4b0740790fd\"\n",
    "wandb.login()\n",
    "\n",
    "# S3 client\n",
    "boto3_session = boto3.Session(profile_name=\"dev\")\n",
    "# create s3 client config object with max retries set to 10 and connection pool size set to 100 with region_name set to us-east-1\n",
    "transfer_config = TransferConfig(\n",
    "    use_threads=True,\n",
    "    max_concurrency=100,\n",
    ")\n",
    "s3_client = boto3_session.client(\"s3\", region_name=boto3_session.region_name)\n",
    "# set connection pool size to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"IM_SIZE\": 128,\n",
    "    \"DROPOUT_RATE\": 0.1,\n",
    "    \"N_EPOCHS\": 15,\n",
    "    \"REGULARIZATION_RATE\": 0.01,\n",
    "    \"N_FILTERS\": 6,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 1,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_DENSE_1\": 2048,\n",
    "    \"N_DENSE_2\": 1024,\n",
    "    \"N_DENSE_3\": 256,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"CHANNELS\": 3,\n",
    "    \"CLASS_NAMES\": [\"REAL\", \"GAN\", \"DM\", \"SD\", \"MD\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(samples):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    index = 1\n",
    "    for image, label in samples:\n",
    "        plt.subplot(4, 4, index)\n",
    "        plt.imshow(image)\n",
    "        title = CONFIGURATION[\"CLASS_NAMES\"][int(label)]\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        index += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_RUN:\n",
    "    # shutil.rmtree(dataset_cache_path, ignore_errors=True)\n",
    "    # delete cache files if they exist\n",
    "    # shutil.rmtree(f\"{dataset_cache_path}*\", ignore_errors=True)\n",
    "    # delete files with glob pattern  if they exist\n",
    "    for file in Path(dataset_cache_path).glob(\"*\"):\n",
    "        file.unlink()\n",
    "\n",
    "\n",
    "# # read training dataset from disk\n",
    "# training_dataset = tf.data.Dataset.load(training_dataset_path)\n",
    "# training_dataset = training_dataset.take(100_000)\n",
    "\n",
    "# training_dataset_sharded_path = f\"{training_dataset_path}_sharded\"\n",
    "# # delete the folder if it exists\n",
    "# if CLEAN_RUN:\n",
    "#     shutil.rmtree(training_dataset_sharded_path, ignore_errors=True)\n",
    "\n",
    "# Path(training_dataset_sharded_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # save the dataset as sharded TFRecord files\n",
    "# def get_shard_id(image, label):\n",
    "#     # generate rnadom number between 0 and 99 using tensorflow random generator\n",
    "#     random_number = tf.random.uniform(shape=[], minval=0, maxval=100, dtype=tf.int64)\n",
    "#     return random_number\n",
    "\n",
    "# training_dataset_sharded = training_dataset.save(path=training_dataset_sharded_path, shard_func=get_shard_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read sharded dataset from disk\n",
    "# loaded_training_dataset_sharded = tf.data.Dataset.load(training_dataset_sharded_path)\n",
    "# # visualize the dataset\n",
    "# visualize_dataset(loaded_training_dataset_sharded.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('/Users/skoneru/workspace/discovery/playground/ai_or_not/cache/models/model_ev2s_99_acc_rgb/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read s3 training dataset path using tensorflow dataset api\n",
    "training_path = f\"s3://{S3_BUCKET}/{PREFIX}training_dataset\"\n",
    "log.info(f\"Reading training dataset from s3: {training_path}\")\n",
    "# s3_training_dataset = tf.data.TFRecordDataset(\n",
    "#     training_path,\n",
    "#     num_parallel_reads=10,\n",
    "#     buffer_size=10_000_000\n",
    "# )\n",
    "\n",
    "\n",
    "def custom_reader_func(datasets):\n",
    "    return datasets.interleave(\n",
    "        lambda x: x, cycle_length=10, block_length=1024, num_parallel_calls=10, deterministic=False\n",
    "    )\n",
    "\n",
    "\n",
    "# images_dataset = tf.data.TFRecordDataset.load(\n",
    "#     path=training_path,\n",
    "#     reader_func=custom_reader_func\n",
    "# )\n",
    "\n",
    "# # gather all the tfrecord files in the directory training_dataset_path\n",
    "tf_record_files = tf.io.gfile.glob(f\"{training_path}/*.tfrecord\")\n",
    "log.info(f\"Found {tf_record_files} tfrecord files\")\n",
    "s3_training_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=tf_record_files, compression_type=COMPRESSION_TYPE, num_parallel_reads=8, buffer_size=1_000_000\n",
    ")\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image_tensor = example[\"image\"]\n",
    "    image_tensor = tf.io.parse_tensor(image_tensor, out_type=tf.float32)\n",
    "    label = example[\"label\"]\n",
    "\n",
    "    return image_tensor, label\n",
    "\n",
    "\n",
    "images_dataset = s3_training_dataset.map(\n",
    "    map_func=parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False\n",
    ").batch(64)\n",
    "\n",
    "# wrire a function to process a batch of examples read from the tfrecord files and return a batch of images and labels tensors\n",
    "# def process_batch_examples(examples):\n",
    "#     images, labels = [], []\n",
    "#     # for example in examples:\n",
    "#     feature_description = {\n",
    "#         'image': tf.io.FixedLenFeature([], tf.string),\n",
    "#         'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "#     }\n",
    "#     parsed_examples = tf.io.parse_example(examples, feature_description)\n",
    "#     # log.info(f\"parsed_examples: {parsed_examples}\")\n",
    "\n",
    "#     images = tf.io.parse_tensor(parsed_examples['image'], out_type=tf.float32)\n",
    "#     labels = parsed_examples['label']\n",
    "\n",
    "\n",
    "#     return images, labels\n",
    "\n",
    "\n",
    "# images_dataset = s3_training_dataset.batch(64).map(\n",
    "#     map_func=process_batch_examples,\n",
    "#     num_parallel_calls=tf.data.AUTOTUNE,\n",
    "#     deterministic=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = images_dataset.take(1)\n",
    "# print(response)\n",
    "# visualize_dataset(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dataset = images_dataset.prefetch(buffer_size=tf.data.AUTOTUNE).cache(filename=dataset_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a keras model from directory\n",
    "tfds.benchmark(images_dataset, batch_size=64, num_iter=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(images_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
